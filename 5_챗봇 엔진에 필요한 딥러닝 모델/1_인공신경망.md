# 인공 신경망

인공 신경망(Artificial Neural Network)은 두뇌의 신경세포인 <a href='https://www.google.com/search?q=%EB%89%B4%EB%9F%B0&oq=%EB%89%B4%EB%9F%B0&aqs=chrome..69i57j0i512l2j0i131i433i512j0i512l3j46i175i199i512j0i512j46i175i199i512.1096j0j7&sourceid=chrome&ie=UTF-8'>뉴런</a>을 수학적으로 모방한 모델이다. 간단히 신경망이라고 부르기도 한다.

우리 두뇌는 뉴런이 100조 개 이상의 시냅스(뉴런과 뉴런을 연결하는 역할)를 통해 망을 구성하고 있다. <br/>
각 뉴런은 다른 뉴런에게 입력 신호를 받아 일정 크기 이상의 신호인지 확인한다.<br/>
이때 임계치를 넘어서면 다른 뉴런으로 신호를 보내는 형태로 구성되어 있다. <br/>
이렇게 연결되어 있는 뉴런에 의미 있는 신호가 들어오면 그 신호에 반응하는 출력 신호를 보내도록 구성되어 있다.<br/>
인공 신경망 역시 이와 동일한 방법으로 모델링되어 있다. <br/>
인공신경망 뉴런에 들어온 입력값이 임계치를 넘어 활성화되면 다음 뉴런으로 출력값을 보낸다.<br/>

### 인공 신경망 뉴런 모델의 수학적 표현

인공 신경망 뉴런 모델의 수학적
![image](https://user-images.githubusercontent.com/66985977/223608174-3e8ddf16-a334-4a3a-906f-e8861ea4b00b.png)

<details>
<summary>수식 참고 <접기/펼치기></summary>
<div markdown="1">
  
![image](https://user-images.githubusercontent.com/66985977/223595241-e3a8aa69-509e-42be-9638-3c1e2384a449.png)
  
</div>
</details>


그림에서는 4개의 입력값을 받아 1개의 출력값을 내보내고 있다. <br/>
뉴런의 출력 수는 항상 1이며, 입력 수는 해결하려는 문제에 따라 임의로 정할 수 있다. (출력 수가 더 필요한 겅우에는 뉴런 수를 늘리면 된다.) <br/>
즉, 인공 신경망에서는 뉴런이라는 함수에 입력값 𝒳₁, 𝒳₂, 𝒳₃, 𝒳₄를 집어넣었을 때 어떠한 방식으로 계산된 출력값 1개가 나온다 생각하면 된다. <br/>
여기서 어떠한 방식은 뉴런이 수학적으로 동작하는 방식을 의미하며, 해당 뉴런이 수학적으로 어떻게 동작하는지 정확하게 알고 있을 필요는 없다. ~~물론 100%다 이해한다면 좋겠지만~~ <br/>
케라스에서 함수 호출 형태로 쉽게 사용할 수 있기 때문에 개념만 이해하면 크게 문제될 것이 없다.

뉴런에 값 (𝒳₁, 𝒳₂, 𝒳₃, 𝒳₄)이 입력되고 있다. 뉴련에는 입력 개수만큼의 가중치 (W₁, W₂, W₃, W₄)와 1개의 편향값(b)를 가지고 있다. 이는 뉴런의 동작 특성을 나타내는 중요한 파라미터로서 이 값들은 조정해 원하는 출력값을 만들어낸다. 실제 정답에 근사하는 출력값을 만들기 위해 뉴런의 가중치와 편향값을 반복적으로 조정한다. 이런 반복적인 과정을 학습이라고 한다.

다시 뉴런의 계산 과정을 입력된 𝒳₁값들과 대응되는 뉴런의 가중치 W₁값들은 각각 곱해서 모두 더해준다. 그리고 편향값 b를 통해 결괏값을 조정한다.<br>
수식 표현 -> ` y = (W₁𝒳₁ + W₂𝒳₂ + W₃𝒳₃) + b `

실제 뉴런은 입력된 신호가 특정 강도 이상일 때만 다음 뉴런으로 신호를 전달한다고 했다.
인공 신경망의 뉴런에서도 동일한 역할을 하는 영역이 있다.(f로 표시된 영역) 이를 활성화 함수라 부르며, 가중치 계산 결괏값 y가 최종적으로 어떤 형태의 출력값으로 내보낼지 결정한다. 활성화 함수에는 여러 종류가 있으며, 출력 형태에 따라 선택하면 된다. 가장 유명한 3가지 활성화 함수가 있는데 활성화 함수의 수식은 깊게 고민하지 않고 함수 그래프 모양과 어떻게 활용될 수 있는지만 확인하면 된다.
  
## 스텝 함수(step function)

![image](https://user-images.githubusercontent.com/66985977/223622252-78fcd6ee-6c89-4d27-b60f-455b563a7970.png) <br/>

이 함수는 그래프 모양이 계단과 같아 스텝 함수라고 부른다. 스텝 함수는 입력값이 0보다 클 때는 1로, 0 이하일 때는 0으로 만든다. 즉, 입력값이 양수일 때만 활성화한다.

### 스텝 함수 그래프 

![image](https://user-images.githubusercontent.com/66985977/223622312-87076cf2-2941-4620-8836-d4313daee941.png)
  
그림은 스텝 함수의 결과를 그래프로 그린 것이다. X축 스텝 함수의 입력밧을, Y축은 스텝 함수의 결괏값을 나타낸다.
Y축은 스텝 함수의 결괏값은 0 또는 1이다. 입력값에 대해 판별해야 하는 결과가 합격/불합격, True/Flase 등 이진 분류 문제일 때 사용한다.
 
하지만 스텝 함수는 결과를 너무 극단적으로 나누기 때문에 실제로 사용하기엔 조금 문제가 있다. 예를 들어 0.1의 경우 0에 가깝지만 0보다 크기 때문에 무조건 1로 출력된다. 이런 경우엔 확률을 이용해서 결과를 소수점으로 표현하는 것이 훨씬 자연스럽다. 이진 분류에서 이런 문제를 해결하기 위해 사용하는 활성화 함수는 시그모이드 함수이다. 
시그모이드 함수는 스텝 함수에서 판단 기준이 되는 임계치 부근의 데이터를 고려하지 않은 문제를 해결 하기 위해 계단 모양을 완만한 형태로 표현했다.
  
## 시그모이드 함수(sigmoid function)

![image](https://user-images.githubusercontent.com/66985977/223624430-00dcf1ed-5133-4198-be65-eb843ba67fe2.png) <br/>
  
### 시그모이드 함수 그래프

![image](https://user-images.githubusercontent.com/66985977/223624630-26bc4dc9-626a-4dd3-a6e9-3ad8e2a83497.png) <br/>
  
X축은 시그모이드 함수의 입력값, Y축은 시그모이드 함수의 결괏값을 나타낸다. 시그모이드 함수의 경우 0에서 1까지의 출력값이 표현된다. 따라서 합격일 확률, 거짓일 확률등으로 표현할 수 있다. 입력한 값을 성공 또는 실패로 나누는 문제에서 시그모이드 함수를 사용한다면 이때 입력값이 0.2인 경우 시그모이드 함수의 출력값은 *0.54정도 된다. 성공 판정의 기준이 되는 *임계치를 0으로 봤을 때 0.2는 성공일 확률을 약 54% 정도로 생각할 수 있다. 하지만 시그모이드 입력값이 커질수록 미분값이 0으로 수렴하게 된다는 단점이 있다.

가중치와 편향을 조정하는 과정이 모델 학습이고 이때 가중치와 편향을 조정하기 위해 사용하는 수학적 도구가 미분이다. 시그모이드 함수 특성상 신경망이 깊어질수록 최종 미분값이 0으로 수렴할 수밖에 없어 학습이 잘 안된다는 사실만 알고 있으면 된다. 또한 시그모이드 함수의 분모에 exp 함수를 사용한다. exp 함수는 연산 비용이 크다. (이때 연산 비용이 크다는 것은 컴퓨터가 이를 계산하는 데에 시간과 자원이 많이 필요하다는 것을 의미) 이런 단점 때문에 최근에는 신경망을 여러 계층으로 구현하는 경우 잘 사용하지 않는다.
  
<details>
<summary>* 임계치 <접기/펼치기></summary>
<div markdown="1">
  
이때 임계치는 결과를 판단하기 위한 기준 값을 의미하고 시그모이드 함수는 0과 1 사이의 값을 출력하므로, 이진 분류 문제에서는 0.5를 기준으로 0.5보다 크면 양성(성공), 0.5보다 작으면 음성(실패)으로 분류하는 것이 일반적입니다. 그리고 0.5를 기준으로 분류하면 양성과 음성을 동일한 비중으로 분류하게 됩니다.

하지만 성공 판정의 기준을 0.5보다 크게 할 수도 있고, 작게 할 수도 있습니다. 이는 임계치(threshold)를 조절하여 결정됩니다.

그러나 일반적으로 성공 판정의 기준을 0으로 두는 것이 좋은 이유는, 이를 기준으로 하면 입력값이 양수(성공)인지, 음수(실패)인지 판단하기 쉽기 때문입니다.

0을 기준으로 한다면, 시그모이드 함수의 출력값이 0.5보다 크면 양성으로, 0.5보다 작으면 음성으로 분류할 수 있습니다. 이를 기준으로 분류하면 입력값이 0보다 크면 양성(성공), 0보다 작으면 음성(실패)으로 판단할 수 있습니다.

또한, 성공과 실패가 동일한 비중으로 나타나는 경우에는 성공 판정의 기준을 0으로 두는 것이 일반적으로 가장 합리적인 선택입니다.
  
</div>
</details>
  
<details>
<summary>* 0.54 <접기/펼치기></summary>
<div markdown="1">
  
시그모이드 함수에서 입력값이 0.2 일때
  
![image](https://user-images.githubusercontent.com/66985977/223632776-0712e580-b664-4b34-ab55-aa81b744c042.png) <br/>
  
</div>
</details>
  
## ReLU(Recified Linear Unit) 함수

![image](https://user-images.githubusercontent.com/66985977/223633473-ed5d7ee9-2d58-4da6-8d7b-a24186ed59ed.png) <br/>
  
최근 들어 활성화 함수로 가장 많이 사용되는건 ReLU 함수다.

### ReLU 함수 그래프

![image](https://user-images.githubusercontent.com/66985977/223633583-2ea07cad-ebca-4810-8104-9246064bb7f1.png) <br/>

ReLU 함수의 구성은 매우 간단하다. 입력값이 0 이상인 경우에는 기울기가 1인 직선이고, 0보다 작을 땐 결괏값이 0이다. 시그모이드 함수에 비해 연산 비용이 크지 않아 학습 속도가 빠르다. 또한 시그모이드 함수의 문제를 완하시키는 데 효과적이라 뉴런의 활성화 함수로 많이 사용하고 있다.

실제로 문제를 신경망 모델로 해결할 때는 1개의 뉴런만 사용하지 않는다. 문제가 복잡할수록 뉴런 수가 늘어나야 하며 신경마의 계층도 깊어져야 한다. 입력층과 출력층으로만 구성되어 있는 단순한 신경망을 심층 신경망(Deep Neural Network)이라 한다. 우리가 흔히 이야기 하는 딥러닝과 신경망은 심층 신경망을 의미한다. 신경망 계층이 깊게(deep) 구성되어 각각의 뉴런을 학습(learning)시킨다 하여 딥러닝 모델이라 부른다. 

### 단층 신경망과 심층 신경망
  
![image](https://user-images.githubusercontent.com/66985977/223881680-8cdaf8ff-a492-41c6-93a8-2bca9b0d6d25.png) <br/>

주로 입력층을 구성하는 뉴런들은 1개의 입력값을 가지며 가중치와 활성화 함수를 갖고 있지 않아 입력된 값 그대로 출력되는 특징을 가지고 있습니다. 출력층의 뉴런은 각각 1개의 출력값을 갖고 있으며, 지정된 활성화 함수에 따른 출력 범위를 가지고 있다. 복잡한 문제일수록 뉴런과 은닉층 수를 늘리면 성능이 좋아진다고 알려져 잇지만 계산해야 하는 파라미터가 많아지면 학습이용이 올라가는 단점이 있습니다. 문제 난이도에 비해 은닉층이 너무 깊어지거나 뉴런 수가 많은 경우에 학습이 잘 안되는 문제도 있으니 많은 실험을 통해 최적의 결과를 내는 쪽으로 결정해야 한다.

### 신경망 학습에 대해서
  
이 부분은 자세하게 파고들면 수학적인 내용이 너무 많이 나온다. 그렇기 때문에 신경망 모델이 어떤 방법으로 학습하는지 개념 정도만 파악하면 될 것 같다.

` 참고_ 뉴런과 노드는 같은 의미이다. `

해당 신경망 모델에서 입력층으로부터 출력층까지 데이터가 순방향으로 전판되는 과정을 순전파(forward propagation)라 한다. 데이터가 순방향으로 전파될 때 현 단계 뉴런의 가중치와 전 단계 뉴런의 가중치와 전 단계 뉴런의 출력값의 곱을 입력값으로 받는다. 이 값은 다시 활성화 함수를 통해 다음 뉴런으로 전파된다. 최종적으로 출력층에서 나온 결괏값(yout)이 모델에서 예상한 결과이다.

### 신경망의 순전파
  
![image](https://user-images.githubusercontent.com/66985977/223888795-faab086a-205e-45af-8bf3-d690634a4d73.png) <br/>
  
앞의 그림에서 이미 학습이 완료돼 최적의 가중치를 찾았다면 해당 결괏값은 모델의 예측값으로 활용할 수 있다. 하지만 우리가 목표하는 실젯값(taget)과 비교해 오차가 많이 발생했다면 다음 순전파 진행 시 오차가 줄어드는 방향으로 가중치(W<sub>h</sub>, W<sub>y</sub>)를 역방향으로 갱신해나간다. 이 과정을 역전파(back propagation)라고 한다.
  
### 신경망의 역전파
  
![image](https://user-images.githubusercontent.com/66985977/223889126-7f07cb09-7203-40ae-9c41-fb897701a018.png) <br/>

최종적으로 나온 순전파 결괏값과 우리가 목표하는 실젯값의 차이를 오차라고 한다. 성능 좋은 모델이란 순전파의 결괏값과 실젯값의 차이가 크지 않은 모델을 의미한다. 즉, 오차가 작은 모델일수록 예측 정확도가 높다. 그럼 오차가 발생했을 때 각 뉴런의 가중치를 어떻게 조정하냐면 가중치 조정은 순전파의 역방향으로 진행된다. 이때 각 단계별로 만나는 뉴턴의 가중치가 얼마만큼 조정되어야 오차를 줄일 수 있는지 계산해 가중치를 개신한다(편미분을 통해 오차가 줄어들 수 있는 가중치 변화 방향의 크기를 계산한다) 이런 일련의 과정을 역전파(back propagation)라 부르며, 딥러닝 모델에서 학습이란 역전파를 이용해 오차를 최대한 줄일 수 있도록 가중치를 조정하는 과정을 의미한다.
  
` 참고_ 시그모이드 함수의 경우 학습 시 층이 깊어질수록 미분값이 0으로 수렴한다. 역전파 진행 시 편미분을 통해 오차가 줄어들 수 있는 가중치 변화 방향의 크기를 계산한다. 이때 미분값이 0이면 가중치 변화가 생기지 않는다. 즉, 가중치가 갱신되지 않아 학습이 되지않는 문제가 발생한다. 따라서 심층 신경망의 원할한 학습을 위해 내부 은닉층의 활성화 함수에는 ReLU를 많이 사용하며, 0에서 1까지 확률적인 표현을위해 출력층에만 시그모이드 함수를 사용해 정확도를 올린다. `
