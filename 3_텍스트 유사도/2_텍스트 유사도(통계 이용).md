# 텍스트 유사도(통계 이용)

챗봇 엔진에 입력되는 문장과 시스템에서 해당 주제의 답변과 연관되어 있는 질문이 얼마나 유사한지 계산할 수 있어야 적절한 답변을 출력할 수 있다.
두 문장 간의 유사도를 계산하기 위해서는 문장 내에 존재하는 단어들을 수치화해야 한다. 이때 언어 모델에 따라 통계를 이용하는 방법과 인공 신경망을 이용하는 방법으로 나눌 수 있으며, 인공 신경망 방법은 <a href='https://github.com/StreetStudy/ChatBot/blob/main/2_%EC%9E%84%EB%B2%A0%EB%94%A9/3_Word2Vec.md'>3_Word2Vec</a>와 같으며 이글은 통계를 이용한 방법이다.

인공 신경망 방식의 성능이 절대적으로 좋은 건 아니다. 상황에 따라서 통계적인 방식이 더 적절할 수 있고, 개발하려는 챗봇의 주제(대표적으로 FAQ에 응대하는 Q&A)에 따라 사용하면 챗봇 엔진 성능 향상에 도움이 된다.

## n-gram 유사도

n-gram은 주어진 문장에서 n개의 연속적인 단어 시퀀스(단어 나열)를 의미한다. n-gram은 문장에서 n개의 단어를 토큰으로 사용한다.
이는 이웃한 단어의 출현 횟수를 통계적으로 표현해 텍스트 유사도를 계싼하는 방법이다. 손쉬운 구현 방식에 비해 학습 말뭉치 품질만 좋다면 괜찮은 성능을 보여준다.
서로 다른 문자을 n-gram으로 비교하면 단어의 출현 빈도에 기반한 유사도를 계산할 수 있으며 이를 통해 논문 인용이나 도용 정도를 조사할 수 있다.

### n에 따른 n-gram

![image](https://user-images.githubusercontent.com/66985977/223298858-958b7dd7-723a-4f25-b5a2-d503c1446324.png)<br/>

(n이 1인 경우 1-gram 또는 유니그램, 2인 경우 2-gram 또는 바이그램, 3인 경우 3-gram 또는 트라이그램, 4이상은 숫자만 앞쪽에 붙여 부른다.)

해당 문장을 n-gram으로 토큰을 분리한 후 단어 문서 행렬(Term-Document Matrix, TDM)을 만든 이후, 두 문장을 서로 비교해 동일한 단어의 출현 빈도를 확률로 계산해 유사도를 구할 수 있다.

### 두 문장을 A와 B로 표현했을 때 B가 A와 얼마나 유사한지 확률을 구하는 수식
![image](https://user-images.githubusercontent.com/66985977/223301087-03c39222-dab2-4512-9517-89fdba1db527.png)<br/>

tf(term frequency)는 두 문장 A와 B에서 동일한 토큰의 출현 빈도를 뜻하며, tokens는 해당 문자에서 전체 토큰 수를 의미한다. 여기서 토큰이란 n-gram으로 분리된 단어이다.
즉, 기준이 되는 문장 A에서 나온 전체 토큰 중에서 A와 B에 동일한 토큰이 얼마나 있는지 비율로 표현한 수식이다. 1.0에 가까울수록 B가 A에 유사하다고 볼 수 있다.

![image](https://user-images.githubusercontent.com/66985977/223301552-a8dde943-6ea3-4252-9418-d7db2fdde84b.png)<br/>


## 2-gram 유사도 계산

```

from konlpy.tag import Komoran


# 어절 단위 n-gram
# 어절 단위로 n-gram 토큰을 추출하는 메서드
# 추출된 토큰들은 튜플 형태로 반환
# 슬라이싱을 이용해 문장을 어절 단위로 n개씩 끊어서 토큰을 저장한다.
def word_ngram(bow, num_gram):
    text = tuple(bow)
    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]
    return tuple(ngrams)


# 음절 n-gram 분석
# 음절 단위로 n-gram 토큰을 추출하는 메서드
# 추출된 토큰들은 튜플 형태로 반환
# 슬라이싱을 이용해 문장을 어절 단위로 n개씩 끊어서 토큰을 저장한다.
def phoneme_ngram(bow, num_gram):
    sentence = ' '.join(bow)
    text = tuple(sentence)
    slen = len(text)
    ngrams = [text[x:x + num_gram] for x in range(0, slen)]
    return ngrams


# 유사도 계산
# doc1의 토큰이 doc2의 토큰과 얼마나 동일한지 횟수를 카운트한다.
# 카운트된 값을 doc1의 전체 토큰 수로 나누면 유사도가 계산된다.
# 이때 결과가 1.0에 가까울수록 doc1과 유사해해진다.
def similarity(doc1, doc2):
    cnt = 0
    for token in doc1:
        if token in doc2:
            cnt = cnt + 1

    return cnt/len(doc1)


sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'
sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'
sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'

komoran = Komoran()
# Komoran 형태소 분석기를 이용해 정의된 문장에서 명사를 리스트 형태로 추출한다.
bow1 = komoran.nouns(sentence1)
bow2 = komoran.nouns(sentence2)
bow3 = komoran.nouns(sentence3)

doc1 = word_ngram(bow1, 2)
doc2 = word_ngram(bow2, 2)
doc3 = word_ngram(bow3, 2)

print(doc1)
print(doc2)
print(doc3)

r1 = similarity(doc1, doc2)
r2 = similarity(doc3, doc1)
print(r1)
print(r2)

```

### 결과

![image](https://user-images.githubusercontent.com/66985977/223322241-31f3174c-85b3-4f5e-99fb-a072ebe27da1.png) <br/>

n-gram은 문장에 존재하는 모든 단어의 출현 빈도를 확인하는 것이 아니라 연속되는 문장에서 일부 단어만 확인하다 보니 전체 문장을 고려한 언어 모델보다 정확도가 떨어질 수 있다.
n-gram의 경우 n을 큭 잡을수록 비교 문장의 토큰과 비교할 때 카운트를 놓칠 확률이 커진다. 하지만 n을 작게 잡을수록 카운트 확률은 높아지지만 문맥을 파악하는 정확도는 떨어질 수밖에 없는 구조이므로 n-gram 모델에서 n의 설정은 매우 중요하며, 보통 1~5사이의 값을 많이 사용한다.

### 정리

일반적으로 학습 데이터가 작을 땐 음절 단위의 n-gram 모델이 유리하지만 문맥을 파악하는 정확도는 떨어질 수밖에 없는 단점이 존재한다.

## 코사인 유사도

단어나 문장을 벡터로 표현할 수 있다면 벡터 간 거리나 각도를 이용해 유사성을 파악할 수 있다. 
벡터 간 거리를 구하는 방법은 다양하지만 그 중 코사인 유사도(cosine similarity)가 존재한다. 

코사인 유사도는 두 벡터 간 코사인 각도를 이용해 유사도를 측정하는 방법이다.
일반적으로 코사인 유사도는 벡터의 크기가 중요하지 않을 때 그 거리를 측정하기 위해 사용한다.

예를 들어 단어의 출현 빈도를 통해 유사도를 계산을 한다면 동일한 단어가 많이 포함되어 있을수록 벡터의 크기가 커진다. 이때 코사인 유사도는 벡터의 크기와 상관없이 결과가 안정적이다.
n-gram의 경우 동일한 단어가 문서 내에 자주 등장하면 유사도 결과에 좋지않은 영향을 미칠 수밖에 없다. 코사인 유사도는 다양한 차원에서 적용 가능해 실무에서 많이 사용한다.

코사인은 -1~1 사이의 값을 가지며, 두 벡터의 방향이 완전히 동일한 경우에는 1, 반대 방향인 경우에는 -1, 두 벡터가 서로 직각을 이루면 0의 값을 가진다.
즉, 두 벡터의 방향이 같아질수록 유사하다 볼 수 있다. 

### 코사인 유사도 수식 ( 행렬곱 )

![image](https://user-images.githubusercontent.com/66985977/223336277-62974eb6-b56c-4409-8e6d-8d391821692f.png)

### 코사인 유사도 수식 계산 과정

예시 비교 데이터 ) <br/>
    A : 6월에 뉴턴은 선생님의 제안으로 트리니티에 입학했다. <br/>
    B : 6월에 뉴턴은 선생님의 제안으로 대학교에 입학했다. <br/>


![image](https://user-images.githubusercontent.com/66985977/223358230-d2c94f50-1b3d-40e7-b80d-be043564f0a1.png) <br/>

해당 그림은 문장 A와 B에서 단어 토큰(명사)만 추출하여 단어 문서 행렬을 표현한 표이다.
열에 나열된 토큰들은 문장 A와 B에 얼마만큼 출현되는지 나타내고 있다. 각 문장에서 토큰이 나올 때 마다 +1씩 카운트했다.

### 문장 A와 B의 벡터는 다음과 같이 정의되었다.

A = [1,1,1,1,1,1,0] <br/>
B = [1,1,1,1,0,1,1]

### 코사인 유사도 수식의 분자 계산

우선 앞의 코사인 유사도 수식의 분자를 계산한다. 코사인 유사도 수식의 분자는 두 벡터의 내적을 의미한다.

![image](https://user-images.githubusercontent.com/66985977/223364780-57d5fc26-4b35-4063-8050-78ab76eaf13e.png) <br/>

### 코사인 유사도 수식의 분모 계산

다음으로 코사인 유사도 수식의 분모를 계산한다. 코사인 유사도 수식의 분모는 두 벡터의 크기의 곱을 의미한다.

![image](https://user-images.githubusercontent.com/66985977/223367905-ef68811a-ab51-40dd-bbbb-af7e30ef1f52.png) <br/>

### 코사인 유사도 수식 결과

두 벡터의 내적은 5, 투 벡터의 크기의 곱은 6이 나왔다. 이 계산 값을 이용해 코사인 각도를 계산하면 0.83이 나온다.
즉, 문장 A와 B는 83%의 유사성을 갖고 있다.

![image](https://user-images.githubusercontent.com/66985977/223369301-313a39fb-c506-405f-80f0-9178d8686d1b.png) <br/>

## 코사인 유사도 계산

```

from konlpy.tag import Komoran
import numpy as np
from numpy import dot
from numpy.linalg import norm


# 코사인 유사도 계산
# 코사인 유사도 계싼에는 넘파이에서 제공하는 벡터내적을 계산하는 매서드와 노름(norm)을 계산하는 매서드를 이용한다.
# 넘파이 패키지의 dot() 매서드는 인자로 들어온 2개의 넘파이 배열을 내적곱(dot product)한다.
# -> (vec1[0] * vec2[0]) + (vec1[1] * vec2[1]) + ··· + (vec1[n] * vec2[n])
#   -> ex)
#      vec1 = [1 1 1 1 0]
#      vec2 = [1 1 1 0 1]
#      dot(vec1, vec2) = (1 * 1) + (1 * 1) + (1 * 1) + (1 * 0) + (0 * 1) = 3
#
# 넘파이에서는 norm() 메서드를 제공한다. (여기서 노름은 벡터의 크기를 나타내는 수학 용어이다.)
# 벡터의 노름에는 여러 가지 종류가 있지만 코사인 유사도에서는 L2(유클리드 노름)을 주로 사용한다.
# ->  √vec[0]^2 + √vec[1]^2 + ··· + √vec[n]^2
#   -> ex)
#      vec1 = [1 1 1 1 0]
#      norm(vec1) =  √1^2 + √1^2 + √1^2 + √1^2 + √0^2 = 2
def cos_sim(vec1, vec2):
    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))


# TDM 만들기
# 비교 문장에서 추출한 단어 사전을 기준으로 문장에 해당 단어들이 얼마나 포함되어 있는지 나타내는 단어 문서 행렬(TDM)을 만들어주는 메서드
def make_term_doc_mat(sentence_bow, word_dics):
    freq_mat = {}

    for word in word_dics:
        freq_mat[word] = 0

    for word in word_dics:
        if word in sentence_bow:
            freq_mat[word] += 1

    return freq_mat


# 단어 벡터 만들기
# Komoran 형태소 분석기를 이용해 정의된 문장에서 명사를 리스트 형태로 추출한다.
def make_vector(tdm):
    vec = []
    for key in tdm:
        vec.append(tdm[key])
    return vec


# 문장 정의
sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'
sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'
sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'

# 헝태소분석기를 이용해 단어 묶음 리스트 생성
komoran = Komoran()
bow1 = komoran.nouns(sentence1)
bow2 = komoran.nouns(sentence2)
bow3 = komoran.nouns(sentence3)

# 단어 묶음 리스트를 하나로 합침
bow = bow1 + bow2 + bow3

# 단어 묶음에서 중복제거해 단어 사전 구축
word_dics = []
for token in bow:
    if token not in word_dics:
        word_dics.append(token)


# 문장 별 단어 문서 행렬 계산
freq_list1 = make_term_doc_mat(bow1, word_dics)
freq_list2 = make_term_doc_mat(bow2, word_dics)
freq_list3 = make_term_doc_mat(bow3, word_dics)
print(freq_list1)
print(freq_list2)
print(freq_list3)


# 문장 벡터 생성
doc1 = np.array(make_vector(freq_list1))
doc2 = np.array(make_vector(freq_list2))
doc3 = np.array(make_vector(freq_list3))

# 코사인 유사도 계산
r1 = cos_sim(doc1, doc2)
r2 = cos_sim(doc3, doc1)
print(r1)
print(r2)

```
